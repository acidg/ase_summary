%!TEX root = ../summary.tex

\section{From Requirements to System Design}
\subsection{Software Architecture}
Software Architecture is the fundamental organization of a system embodied in its components, their relationships to each other and the environment and the principles guiding its design and evolution.
Its main purposes are quality, the efficiency of the development process, risk minimization, and managing communication and knowledge.\\
Figure~\ref{fig:software_intensive_system_architecture} describes a model of how architecture looks like in software intensive systems.
\begin{figure}[H]
  \centering
  \includegraphics[width=.8\textwidth]{images/software_intensive_system_architecture.png}
  \caption{Architecture in Software Intensive Systems}\label{fig:software_intensive_system_architecture}
\end{figure}

\begin{itemize}
  \item The \textbf{system} is a collection of components which are organized to accomplish a function and has defined boundaries, consists of components and interfaces interacts with its environment through these interfaces and is defined by its static structure and dynamic behavior.
  \item The \textbf{environment} are the developmental, operational, political and other influences on the system.
  \item Every system has an \textbf{architecture} that can be described by an \textbf{architectural description}.
  \item \textbf{Stakeholders} are people that have an interest in the system which can have various roles regarding the architectural description.
  \item \textbf{Concerns} are interests which influence the system's development, its operation or any other aspect that is important to stakeholders.
  \item \textbf{Views} address one or more concerns of the system stakeholders.
  \item A \textbf{viewpoint} then describes a view and any associated modeling methods or analysis techniques by determining the languages for the architectural description
  \item \textbf{(System) models} provide abstractions in different ways.
    The object model describes the structure of the system, the functional model what the system's functions are and a dynamic model how the system reacts to external events.
\end{itemize}

\subsubsection{Modeling}
Models \textbf{reduce} the reality to a subpart of the original where irrelevant parts to the application are omitted which increases abstraction.
This reduction always has a \textbf{purpose} in mind which makes it  adequate for a purpose or not and it is always possible to find a \textbf{mapping} between reality and the model.\\
The process of modeling can be divided in the following repeating sub-processes:
\begin{itemize}
  \item \textbf{Understanding} the application domain and its problems and possible solutions
  \item \textbf{Conceptualize} the part of the domain that is of relevance with the help of a concept language
  \item \textbf{Abstract} by outlining the main problems that have to be supported by the system on the basis of forgetful mappings.
  \item \textbf{Define} the main concepts/annotations used for the development of the model.
  \item \textbf{Construct} a model by organizing and linking ideas, judgments or concepts.
  \item \textbf{Evaluate} the model or parts of it considering predefined quality characteristics.
  \item \textbf{Refine} with iterative development to make the model more elaborated while maintaining its main structure.
\end{itemize}

\subsubsection{Modularity}
Modularity is the decomposition of a system into components to manage complexity by hiding unnecessary information to the outside world in these components.
This increases maintainability and re-usability since the single components can be switched out or used elsewhere.
Also work can be easily distributed to the single components.\\
Functional decomposition decomposes the system regarding functions which implies that one has to understand the whole system to make a change possibly.
A better approach is modular decomposition where modules are the main concepts in a system.
This assumes we can find concepts in a new (greenfield engineering) or existing (re-engineering) software system  and that we can create a component-based interface on any system (interface engineering).\\
When looking at modules, we can either assume a black-box view where we only look at the possible input-output combinations and not the internals of the system, or a white-box view where the internals are regarded.

\subsubsection{Component-based Software Engineering (CBSE)}
CBSE is an approach to software development that relies on the reuse of software components which are a set of classes that can be considered as stand-alone service providers.
Components then interact with each other over published, clear defined and standardized interfaces and are integrated with the help of some middleware.\\
A software component has the following properties:
\begin{itemize}
  \item \textbf{Standardized}: Conformation to a standard component model
  \item \textbf{Independent}: Deployable without dependences to other components or if needed it should be stated in a ``requires'' interface specification.
  \item \textbf{Composable}: All interactions have to happen through the public interface
  \item \textbf{Deployable}: Ability to operate as stand-alone entity on a component platform that provides an implementation of the component. This usually means that binaries are provided so that no compilation is necessary before deployment.
  \item \textbf{Documented}: Syntax and also ideally semantics should be specified
\end{itemize}

Component interfaces are defined as shown in Figure~\ref{fig:component_interfaces}.\\
\begin{figure}[H]
  \centering
  \includegraphics[width=.8\textwidth]{images/component_interfaces.png}
  \caption{Component Interfaces}\label{fig:component_interfaces}
\end{figure}

A component model is a definition of standards for component implementation, documentation and deployment as shown in Figure~\ref{fig:component_model}.
\begin{figure}[H]
  \centering
  \includegraphics[width=.7\textwidth]{images/component_model.png}
  \caption{Component Model}\label{fig:component_model}
\end{figure}
It specifies how and in which language the interfaces should be defined and the elements which should be included (e.g.\ names, parameters,\ldots).
Furthermore naming conventions are defined for the usage of the component, e.g.\ URIs and meta-data is provided which gives information about interfaces, attributes and helps users to find out what services are provided and required.
Lastly the component model specifies how components should be packaged for deployment usually including all dependencies not specified in the ``required'' interface.\\

After the modeling of the components, they can be composed as shown in Figure~\ref{fig:component_composition}.
\newline

\begin{figure}[H]
  \centering
  \includegraphics[width=.7\textwidth]{images/component_composition.png}
  \caption{Component Composition}\label{fig:component_composition}
\end{figure}

\begin{minipage}[t]{0.49\textwidth}
    \textbf{Pros}
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item Independent components
        \item Component standards to facilitate integration
        \item Middleware to provide support for inter-operability
        \item Development process that is geared to reuse
    \end{itemize}
\end{minipage}
\begin{minipage}[t]{0.49\textwidth}
    \textbf{Cons}
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item Component trustworthiness
        \item Component quality certification
        \item Emergent property prediction
        \item Requirements trade-offs (difficult analysis between features of two components)
    \end{itemize}
\end{minipage}

\subsubsection{Design by contract}
Design by contract presents a set of principles to produce dependable and robust object-oriented software.
Thereby a contract is an agreement between the client and the supplier.
Each party expects benefits from the contract and is prepared to incur some obligations to obtain them.
These benefits and obligations are documented in the contract where no obligations than the ones documented can be imposed to a party to obtain the benefits (no hidden clause rule).\\
The design principles design by contract proposes are
\begin{itemize}
  \item \textbf{Non-redundancy}: no tests of preconditions (contrary to defensive programming)
  \item \textbf{Reasonable preconditions}: precondition is written in the documentation and can be justified according to that specification
  \item \textbf{Failure principle}: execution of rescue clauses to its end, not leading to a retry instruction, causes the current routine to fail.
  \item \textbf{Disciplined exception handling}: The two possible reactions to an exception are retrying or a failure/organized panic.
  \item \textbf{Exception simplicity} Simple rescue clauses that only bring the object back to a stable state, permitting a possible retry
\end{itemize}

\subsubsection{Dependency Structure Matrix (DSM)}
A DSM is a two-dimensional matrix representing the structural or functional interrelationships of objects, tasks or teams.
Each entry in the matrix indicates that the item on the corresponding column depends on the item on the corresponding row.\\
Partitioning on the matrix describes the transformation so that all dependencies are below the diagonal or within groups (interdependencies) and results in a block triangular matrix form.
To eliminate the groups/interdependencies, the corresponding rows and columns can be grouped to one (c.f.\ Figure~\ref{fig:dsm_grouping}).
\begin{figure}[h]
  \centering
  \includegraphics[width=.8\textwidth]{images/dsm_grouping.png}
  \caption{DSM Grouping}\label{fig:dsm_grouping}
\end{figure}

DSMs can be used to identify patterns an anti-patterns in software, c.f.\ Figure~\ref{fig:dsm_layered_pattern} and Figure~\ref{fig:dsm_propagator_anti_pattern}.

\begin{figure}
  \centering
  \begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/dsm_layered_pattern.png}
    \captionof{figure}{Layered Pattern in DSM}\label{fig:dsm_layered_pattern}
  \end{minipage}%
  \begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/dsm_propagator_anti_pattern}
    \captionof{figure}{Propagator Anti-Pattern in DSM}\label{fig:dsm_propagator_anti_pattern}
  \end{minipage}
\end{figure}

\begin{minipage}[t]{0.49\textwidth}
    \textbf{Pros}
    \begin{itemize}[topsep=0pt, itemsep=0pt]
      \item Better scaling than box-and-line diagrams
      \item Better understanding of information flows
      \item Automatic partitioning algorithms
      \item Efficient cycle detection
      \item Integration of dependency rules
    \end{itemize}
\end{minipage}
\begin{minipage}[t]{0.49\textwidth}
    \textbf{Cons}
    \begin{itemize}[topsep=0pt, itemsep=0pt]
      \item Only as good as the knowledge that goes in (unknown dependencies might exist)
      \item less intuitive than a graph
    \end{itemize}
\end{minipage}

\subsubsection{Guidelines for Modular Design}
\paragraph{Low Coupling and High Cohesion}
We denote the structure of a system as $S = (C,I,CON)$ where C are the components, $env \in C$ the environment, I the interfaces and $CON \subseteq I x I$ is the connection between interfaces.
Figure~\ref{fig:system_structure_notation} shows the notation and possible relationships between components.
\begin{figure}[h]
  \centering
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/component_parent_relationship.png}
    \caption{Parent relationship\\ $parent(c) = parent(d) = a$\\ $parent(a) = env$}
  \end{subfigure}
  \hspace{.03\textwidth}
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/component_interface_relationship.png}
    \caption{Interface-Component Relationship\\ $assigned(i) = a$\\ $assigned(k) = b$}
  \end{subfigure}
  \hspace{.03\textwidth}
  \begin{subfigure}{.3\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/component_connection.png}
    \caption{Connection between Interfaces\\ $connected(c) = (i,k)$}
  \end{subfigure}
  \caption{System Structure Notation}\label{fig:system_structure_notation}
\end{figure}

We then define \textbf{coupling} as the normalized number of connections between components at the same hierarchical level 
\begin{equation*}
  coupling(s) = \frac{|\{con \in s.CON| \exists i,j \in s,I: con = connected(i,j) \wedge parent(assigned(i)) = parent(assigned(j))\}|}{|s.C|}.
\end{equation*}
The goal is to make coupling as low as possible.

We can define different classes of coupling, in the following listed in decreasing amount of coupling order:
\begin{itemize}
  \item \textbf{Content Coupling}: One component changes another component's data or when control is passed from one component to the middle of another.
  \item \textbf{Common Coupling}: Two components share data which shows a lack of clear responsibility and reduces readability, maintainability and reusability
  \item \textbf{External Coupling}: Components communicate through an external medium such as a file, device interface, protocol or data format.
  \item \textbf{Control Coupling}: One Component directs exaction of another component by passing the necessary control information. This approach can be good if parameters allow factoring and reuse of functionality or bad if parameters indicate completely different behavior or components are not independent.
  \item \textbf{Stamp Coupling}: Complete data structures are passed from one component to another.
  \item \textbf{Data Coupling}: Component passes data (not data structures) to another component.
\end{itemize}

\textbf{Cohesion} is defined as how closely related different responsibilities of a component are. The goal is to make cohesion as high as possible.\\
Like for coupling, we can again define types of cohesion, also in decreasing order:
\begin{itemize}
  \item \textbf{Functional Cohesion}: Every essential element to a computation is contained in the component so that a single input is transformed to a single output. Ideal kind of cohesion in that it maximizes re-usability, testability, understandability, learnability, extensibility and maintainability.
  \item \textbf{Sequential Cohesion}: The output of one component is the input of another and data flows between parts. Quite good kind of cohesion.
  \item \textbf{Communicational Cohesion}: Elements operate on the same data
  \item \textbf{Procedural Cohesion}: Elements are related only to ensure a particular order of execution. Actions are weakly connected and unlikely to be reusable
  \item \textbf{Temporal Cohesion}: Elements are independent but are activated around the same point in time. Code is spread out which results in bad maintainability and re-usability
  \item \textbf{Logical Cohesion}: Elements are logically related, not functional
  \item \textbf{Coincidental Cohesion}: Elements have no significant relation to each other (worst kind).
\end{itemize}

\paragraph{Single Responsibility Principle}
Every class should only have one responsibility, if there are multiple split the class.
A guideline for this is ``there should never be more than one reason to change a class''.

\paragraph{Separation of Concerns}
Separate features to different components which encapsulate a semantic concern what helps humans to handle complexity (only able to hold around 7 things in mind).
This also enables components to be easily replaced.
Modules therefore designed with the \textbf{open/closed principle} in mind.
This means that a module is open for extension but closed for modification so that the behavior cannot be changed but only extended and thus no unexpected effects will arise.

\paragraph{Liskov Substitution Principle (LSP)}
\begin{chapquote}{Barbara Liskov. (1987)}
    Let $q(x)$ be a property provable about objects x of type T. Then $q(y)$ should
be provable for objects y of type S where S is a subtype of T.
\end{chapquote}
This means child classes should never violate the invariants of the base class or remove some of its behavior.

\paragraph{Interface-Segregation Principle}
Prefer small, cohesive interfaces over big ones.

\paragraph{Anticipate change}
Build components in a way that minimizes effort for potential future changes by finding a compromise between generality and specificity.
A good guideline here is the principle of low coupling and high cohesion.
It is easy to overdo it though, since reusability is oftentimes very difficult.

\paragraph{Do not Repeat Yourself}
Every piece of knowledge (and functionality) must have a single, unambiguous, authoritative representation within a system.
This dramatically improves maintainability.\\
Duplication can either be imposed (developers have no choice due to environment), inadvertent (duplication not realized by devs), due to impatience or laziness or due to inter-developer communication or the lack thereof.

\subsubsection{Architecture and External Quality}
When introducing a high degree of modularity we influence internal quality by increasing maintainability and re-usability.
Testability might be increased or decreased depending on the system though.
External quality might suffer also by introducing to many layers or indirections
which decreases performance and also security might suffer by introducing large attack surfaces.\\

\subsection{Antipatterns}
Antipatterns are commonly occurring solutions to a problem that generates decidedly negative consequences.
They define an industry vocabulary for common defective processes and implementations within organizations.
Antipatterns can concern software development and software architecture.\\
The reasons for these mistakes are called the 7 deadly sins:
\begin{itemize}
  \item Haste
  \item Apathy (not caring)
  \item Narrow-Mindedness (not applying common solutions)
  \item Sloth (solutions based on easy answers)
  \item Avarice (excessive complexity)
  \item Ignorance
  \item Pride
\end{itemize}

Common antipatterns are:
\begin{itemize}
  \item The blob: A ``god class'' that does everything typically caused by the lack of an architecture or the enforcement thereof, too limited intervention or by the specification of the requirements. 
  \item Functional decomposition: Modules are based on functionality instead of semantic parts from the view of the user. It is typically caused by a lack of object-oriented understanding, the lack of architecture enforcement and sometimes due to badly specified requirements
  \item Auto-generated stovepipe: Use generators and try to fit the design of the architecture to the capabilities of the generator, e.g.\ trying to use the same interfaces of an existing system for a distributed one.
  \item Golden Hammer: Using the same tools for everything
  \item Design by committee: A complex software design is the product of a committee process of too many people. The design is too complex to realize and test due to excessive complexity, ambiguities, over constraint and other specification defects.
\end{itemize}

\subsection{Reuse}
When developing software, it is usually advisable not to ``reinvent the wheel''.
Therefore, components should be designed with some principles in mind to support reuse:
\begin{itemize}
  \item Modularity to extract and insert modules
  \item Loose coupling high cohesion also to extract modules
  \item Information hiding for plug and play usage
  \item Separation of concerns to be able to tell the responsibilities of a component
\end{itemize}
There are some pitfalls when trying to write reusable components though.
Either technical issues like the reuse is only done at code level, which has a very limited scope and is often done unsystematically, or inconsistent or incomplete component specifications re-usability or organizational reasons like the lack of planning, motivation or component marketplaces can hinder the development of reusable software.
Also every time a trade-off between flexibility and stability has to be met.
On the one hand, flexibility has to be high enough in order to make reuse feasible.
On the other hand, sufficient stability has to be provided in order to generate benefits from the reuse.\\
Reusable components can either be a side product of development (opportunistic or ad-hoc reuse) or be planned (planned or structured reuse).
Ad-hoc reuse usually does not scale due to the lack of infrastructure to maintain components like search, evaluation, adaption or integration services.
The better approach is structured reuse where essential functionality is parameterized so that it can easily be customized usually during deployment.

\subsubsection{Variability}
Variability can either be visible to the user (external variability) or hidden from them (internal variability).
It is supported by the concept of managed variability which implies defining variability, managing variable artifacts, supporting activities concerned with resolving variability and collecting, storing and managing trace information necessary to fulfill these tasks.
The time when variabilities are resolved is called binding time.\\
Defining variability can be done as an integral part of development artifacts, which increases their complexity though, or in a separate model.

\paragraph{Orthogonal Variability}
An orthogonal variability model is a model that defines the variability of a software product line. It relates the variability defined to other software development models such as feature model, use case models, design models, component models, and test models.\\

On requirement level, variability is modeled as feature diagram where parts can be mandatory or optional and have interdependencies.
A variation point is defined as a representation of a variability subject and a variant then is a representation of a variability point.
Figure~\ref{fig:feature_diagram} shows an example of a feature diagram.\\
\begin{figure}[h]
  \centering
  \includegraphics[width=.8\textwidth]{images/feature_diagram.png}
  \caption{Feature Diagram}\label{fig:feature_diagram}
\end{figure}

On code level, variety can be supported with conditional compilation, polymorphism, frame technology or aspect-oriented programming.

\subsubsection{Product Line Engineering (PLE)}
A software product line is a set of applications with a common architecture and shared components, with each application specialized to reflect different requirements.
Specialization can take place in several domains including platform, environment, functions and processes.\\
PLE then is the process of developing a product line with a heavy focus on re-usability by separating development into \textbf{family engineering} and \textbf{application engineering}.
Figure~\ref{fig:product_line_engineering} shows an illustration of this process.
\begin{figure}[h]
  \centering
  \includegraphics[width=.7\textwidth]{images/product_line_engineering.png}
  \caption{Product Line Engineering}\label{fig:product_line_engineering}
\end{figure}
In family or domain engineering product commonalities and variabilities are identified and reusable (product line) artifacts are developed.
An essential part of this is the scoping of the product line by defining sharp boundaries based on most of the concrete product requirements (all is not economical most of the time).
Important aspects of this are which products will be in the line, which domains to consider, which features are required and which assets are in the line.
These base artifacts cover variability as well as commonalities of the PL and are stored in the \textbf{product line artifact base}.
During the application engineering process their variabilities then are instantiated according to the specific product requirements and thus individual products are developed.\\

\begin{minipage}[t]{0.49\textwidth}
    \textbf{Pros of well-done PLE}
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item Reduces development costs
        \item Increases quality
        \item Decreases time-to-market
        \item Better understanding of the domain
        \item Structured reuse possible
    \end{itemize}
\end{minipage}
\begin{minipage}[t]{0.49\textwidth}
    \textbf{Cons}
    \begin{itemize}[topsep=0pt, itemsep=0pt]
        \item High up-front invenstments
        \item Strong domain knowledge and therefore experts necessary
        \item May result in a badly scoped domain what leads to unwanted assets
        \item Limited success in reality
    \end{itemize}
\end{minipage}

\subsubsection{Reference Architectures and Frameworks}
\paragraph{Software Frameworks}
A framework is a set of classes that embodies an abstract design for solutions to a family of related problems, and supports reuse at a larger granularity than classes.
Frameworks support design reuse by providing a skeleton architecture for the application as well as the reuse of specific classes and are language specific.
Applications using a framework extend generic classes of the framework in so called extension points or hot-spots.\\
The main difference to a library is the call hierarchy.
Frameworks call the application specific classes whereas when using a library the application calls the library code.
Also frameworks usually already provide a semi-complete application whereas libraries are only pluggable parts of it.

\paragraph{Reference Architectures}
A reference architecture is an abstract software architecture for a specific application area. It defines structures and types of software elements, and their interactions and responsibilities. The defined structures are applicable for all systems of a domain.
There are three different types of reference architectures described in Figure~\ref{fig:reference_architectures}.
\begin{figure}[H]
  \centering
  \begin{tabular}{|p{.225\textwidth}|p{.225\textwidth}|p{.225\textwidth}|p{.225\textwidth}|}
    \hline
    & Functional & Logical & Technical\\
    \hline
    Phase of the development process & Requirements analysis & Conceptual design & Detailed design, Implementation\\
    \hline
    Provides a basis for & Functional specification, planning of subsystems & Logical architecture, planning of the implementation & Detailed architecture, implementation, deployment\\
    \hline
    Elements & Functional areas as units of functionality & Components as units of design and implementation & Components as units of implementation and deployment\\
    \hline
    Stakeholder & User, manager, project leaders & Project leaders, architects, developers & Architects, developer, maintenance staff\\
    \hline
    Architectural overview & Functional areas, data flow & Components, layers to be implemented & Technical components, layers which can be deployed\\
    \hline
    Textures (structures, principles and design concepts which occur often) & - & Design rule, may be expressed as a design pattern & Design rule, design pattern, code template\\
    \hline
    Reference interfaces & Named interfaces, if any & Named interfaces; defined in an implementation neutral definition language (e.g., IDL), if necessary & Defined in the programming language used\\
    \hline
    Infrastructure & - & Basic properties & Exactly defined\\
    \hline
  \end{tabular}
  \caption{Types and Elements of a reference architecture}\label{fig:reference_architectures}
\end{figure}

\subsection{Testability}
In general testing or the assessment thereof is a very difficult topic to approach since we often do not know what metrics apply for good tests or tests suits and oftentimes that depends on the system under test.\\
For that reason it is also quite difficult to measure the testability of a system, nevertheless we will try to give some definitions.
All testing theory is mostly academic though, despite having a big importance in industry as well.\\

The first approach defines testability as the degree to which a system or component facilitates the establishment of test criteria and the performance of tests to determine whether those criteria have been met.\\

The second one tries to cast the problem to one of information loss which occurs if the input values to a program do not propagate to the output.
This might happen implicitly if many input values yield the same output or explicitly if local variables are not checked after execution.
With this in mind, we then define testability as the domain/range ration $DRR = \frac{|input~domain|}{|output~domain|}$.
If the DRR is large, testability is poor.
In case of implicit losses, we cannot do much since this depends on the specification.
If we have explicit losses, one solution might be to increase the observability by increasing the size of the output domain by explicit observations.
We might also increase the controllability for being able to also change modules internal states.
By increasing observability and controllability, we need information about who modules work which contradicts the ideas of modularity, encapsulation and information hiding though.\\

A last approach then would be to define testability as likelihood of a program to fail with the next test (given a particular assumed input distribution) if the software includes a bug.
Although this might be a better definition for the quality of tests.
With that definition we can try to inject defects into our programm and see if the output changes.
By measuring the likelihood of such a change we can define that as the amount of testability our program has.

\subsection{Safety}
Systems, no matter in what domain, are usually never to 100\% safe in reality.
Therefore we define a safe system as a system that is above a specified safety threshold.
So the goal is to determine how safe is safe enough without over- or under-engineering a product.

\subsubsection*{Risk}
Before we define such a threshold though, we have to define a measurement scale.
For that we will use risk what is defined as the probability of the occurrence of harm times the severity $R = PHarm \cdot SHarm$ and harm as the physical damage to persons or the environment according to IEC 61508 and ISO 26262 (fundamental security standards).\\
What is to note here that risk perception does usually not represent the actual risk.
Risk perception is the subjective judgment that people make about the characteristics and severity of a risk.
Risk aversion then is the reluctance of people to accept a bargain with an uncertain payoff rather than another bargain with more certain, but possibly lower, expected payoff and scale aversion then is the tendency to want greater protection where consequences are high.\\

In reality risk is usually reduced to a level where it is \textbf{as low as is reasonable practical (ALARP principle)}.
This reduction can take place in several aspects of systems: external measures like guardrails besides roads, electronics and software and other technologies like hydraulic back-ups.
The process of risk reduction is depicted in Figure~\ref{fig:risk_reduction_process}.\\
\begin{figure}[H]
  \centering
  \includegraphics[width=.2\textwidth]{images/risk_reduction_process.png}
  \caption{Risk Reduction Process}\label{fig:risk_reduction_process}
\end{figure}

With these definitions in mind we can define safety as freedom or absence of unacceptable or unreasonable risk (complies to IEC and ISO).

\subsubsection*{Faults, Errors and Failures}
\begin{figure}[H]
  \centering
  \includegraphics[width=.8\textwidth]{images/fault_error_failure.png}
\end{figure}
A fault is an abnormal condition that can cause an element or an item to fail.
An error is defined as the discrepancy between a computed, observed or measured value or condition and the true, specified or theoretically correct value or condition and a failure then is the actual termination of the ability of an element to perform a function as required.

Faults will lead to errors but errors not necessarily to failures due to error correction and redundancy.\\
Causes of faults can be random issues like damage of fatigue or systematic issues like specification or design problems.
Random failures can only occur in hardware and can be predicted with reasonable accuracy whereas systematic failures can be in hardware or software and can only be eliminated by a change in design or manufacturing and cannot be predicted.
Latter ones can be tried to be avoided during the design and production phase by using techniques and procedures that aim to avoid the introduction of faults.
Furthermore tolerance during operation can be introduced as mentioned before.
This introduction of tolerance can also be applied for random faults.\\

Failures of components can cause faults in systems which result in errors and maybe failures of the system.
This can happen in a cascading fashion where component A fails, afterwards component B, \ldots until the system fails.
Another way for system failure is a common cause failure where one root cause lets several components fail ``simultaneously'' which might ultimately lead to a system failure.

\subsubsection*{Functional Safety}
Functional safety focusses on the hazards (= potential source of harm) and risks originating from the function of an (E/E) system.

The lifecycle approach in this context is an concept from 1947 which defines: The primary concern of the safety life cycle is the management of hazards: their identification, evaluation, elimination, and control through analysis, design and management procedures.
Based on this concept, multiple functional safety standards arose which describe a safety lifecycle and identify activities and requirements based on it.
An example is shown in Figure~\ref{fig:iso26262_software_lifecycle}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/iso26262_software_lifecycle.png}
  \caption{ISO 26262 Software Lifecycle Model}\label{fig:iso26262_software_lifecycle}
\end{figure}

\subsubsection*{Safety Analyses using FMEA and FTA}
Safety Analyses examine the consequences of faults and failures of functions, behavior and design of elements and provide information on conditions that could lead to the violation of safety goals.
Furthermore they contribute to the identification of new non-functional hazards not discovered before.
This is done on multiple levels of abstraction during the concept and product development phase.\\
Qualitative analysis methods identify failures and do not predict their frequency whereas the quantitative approach does actually predict the frequency e.g.\ with Markov models.
Another classification differentiates inductive vs deductive methods.
Inductive ones (\textbf{failure mode and effects analysis (FMEA)}) are bottom-up methods which start with root causes to forecast unknown effects whereas deductive analysis methods (\textbf{fault tree analysis (FTA)}) go top-down and try to find out root causes from known effects.
FMEA and FTA approaches are shown in Figures~\ref{fig:fmea} and~\ref{fig:fta}.
\begin{figure}[H]
  \centering
  \begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/fmea.png}
    \captionof{figure}{Failure Mode and Effect Analysis}\label{fig:fmea}
  \end{minipage}%
  \begin{minipage}{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{images/fta.png}
    \captionof{figure}{Fault Tree Analysis}\label{fig:fta}
  \end{minipage}
\end{figure}

\paragraph{Design FMEA}
\begin{figure}[H]
  \centering
  \includegraphics[width=\textwidth]{images/design_fmea.png}
  \caption{Design FMEA}
\end{figure}
\begin{description}
  \item[Failure mode] Manner in which a component might fail to meet design intent
  \item[Failure Effects] Effects of the failure mode on the functions perceived by the customer
  \item[Causes] Indication of the design weakness causing the failure mode
  \item[Detection/Prevention Measures] Activities that assure the design adequacy for the failure
  \item[Severity (SEV)] Severity of potential failure effect on a scale from 1 to 10
  \item[Occurence (OCC)] Likelihood of the failure on a scale from 1 to 10
  \item[Detection (DET)] Likelihood of not detecting the failure before reaching the end-user on a scale from 1 to 10
  \item[Risk Priority Number (RPN)] $RPN = SEV \cdot OCC \cdot DET$ used for priorization of concerns and actions
\end{description}

\paragraph{FTA}
Procedure:
\begin{enumerate}
  \item Plan of the system and FMEA (if existing) as input.
  \item Define system under scrutiny
  \item Determine undesired events/failures
  \item Identify event or series of events that lead to top-level event
  \item Apply recursively (apply AND/OR symbols)
  \item Identify cut sets, a set of events that, taken together, lead to system failure.
\end{enumerate}

\subsection{Model-based Engineering}
Models are appropriate abstractions of reality for a particular purpose and are an essential part of managing complexity in software engineering.
Models consist of an operational context, interfaces, behavior at the borders of those interfaces and/or an inner structure of the artifacts in the system design.
While modeling, different viewpoints have to be considered and a distinction between different levels of granularity has to be done.\\

The ISO 26262's component model, shown (partly) in Figure~\ref{fig:iso_component_model}, supports a modular construction of the item under consideration with a suitable levels of abstraction.
\begin{figure}[H]
  \centering
  \includegraphics[width=.8\textwidth]{images/iso_component_model.png}
  \caption{ISO 26262 Component Model}\label{fig:iso_component_model}
\end{figure}
It also supports inter-level allocation and modular decomposition of system models and allows for distributed development and reusable elements.\\

MBE at an logical viewpoint provides a logical architecture consisting of a network of components connected via directed channels representing the communication path and interfaces.
Components reactive behavior is described with state transition diagrams, code specifications and message sequence diagrams and the computational behavior with data flow diagrams.
Hereby a logical description of the solution by providing a platform independent, reusable definition of the complete system behavior is given.\\

On a technical viewpoint, the hardware architecture is composed of hardware components, ports (sensors and actuators) and busses.
The software architecture is divided in a dynamic (tasks, messages, schedulers) and a static part (platform, middleware).
Technical modeling provides a platform specific solution by defining necessary artifacts of software and hardware parts.\\

A deployment logic model is then used to map components from the logical to the technical architecture.
In the end a code generator is used to form an executable program out of the models.\\

Advantages of MBE:\@
\begin{itemize}
  \item Easy verification and validation possible through model based test case generation, simulation,\ldots
  \item Easy quality analysis possible
\end{itemize}

\subsection{Security}
Security is the protection of systems from external hazards.
For recall, safety is the protection of people and the environment from hazards \textbf{originating from the system}.\\
Security includes:\\
\begin{minipage}[t]{0.49\textwidth}
    \begin{itemize}[topsep=0pt,noitemsep]
      \item Confidentiality
      \item Integrity
      \item Availability
      \item Non-repudiation
      \item Authenticity
    \end{itemize}
\end{minipage}
\begin{minipage}[t]{0.49\textwidth}
    \begin{itemize}[topsep=0pt, noitemsep]
      \item Auditability
      \item Accountability
      \item Privacy
      \item Anonymity
    \end{itemize}
\end{minipage}

\subsubsection{Relevance and Challenges}
Information Security might matter in several domains like sensitive private, commercial or government data.
Since in recent years everything gets more connected and more attack angles arise, security might become increasingly important.
On the other hand, measures to protect data are costly and slow down systems, so we have to question ourselves if the overhead is worth it.
This might depend on the specific projects though.
Also, like with safety, we can never reach 100\% security.\\
Security can be enhanced in different domains of a software system or a system in general.
First, technical measures can be taken by cryptography, software and systems engineering or physical technologies (e.g.\ smart cards) amongst others.
But it is also possible to increase security on an organizational, people-related or legal level.
Especially people seem to be the weak link here by using same passwords, ignoring security warnings in the browser or even disabling security measures all together.

\subsubsection{Design Principles}
\begin{description}
  \item[Least privilege] Every subject should not have more privileges than necessary to complete its (approved) job.
    This minimizes the negative consequences of an inadvertent operating error in components.
  \item[Complete mediation] Access to every object must be controlled in a way not circumventable.
  \item[Secure, fail-safe defaults] Security mechanisms should start in a secure state and return to a secure default state in case of failures.
  \item[Compartmentalization] Organize resources into groups (compartments, zones), isolated from others except for limited, controlled means of communication.
    This applies the principle of least privilege to entire components so that problems from attacks are limited to one compartment. Highly security-sensitive resources can even be isolated in special parts.
  \item[Minimum exposure] Minimize the attack surface of potential attacks.
\end{description}

\subsubsection{Implementation Level Concerns}
\paragraph{SQL Injection}
SQL injection can be applied when a server takes a user input as a part of a database query.
These queries often look like ``SELECT fieldlist FROM table WHERE field = '\$INPUT' ''.
Attackers can then enter something along the lines of ``anything' OR 'x'='x '' which actually extends the SQL query to their likes.

\paragraph{Cross-Site Scripting}
XSS can occur in two different forms, namely non-persistand/reflective, persistent or DOM based attacks.\\
Reflective XSS can be carried out if the server takes URL parameters and echos them to the website.
Attackers can place scripts as these parameters which will then be executed if the URL is opened.
A common approach to that is sending a malicious link via E-mail and stealing cookies and thus user data from the receiver when they click the link.\\
Persistent XSS is possible if a website stores user submitted data and displays it to others.
Attackers use this by again placing scripts into the input of the server which stores them and renders them into HTML pages open another user requests a corresponding site.
The script tag, if not escaped, then is inserted as actual script that will be executed.\\
XSS can be prevented by escaping embedded JavaScript, but this is difficult, since the script can be encoded in different ways and other trickeries are possible (<scr<script>ipt> $Rightarrow$ script tag is remove, but another script tag will be there).

\paragraph{Buffer Overflow Attacks}
BOAs use the fact that in the x86 architecture call stack and heap usually are kept in the same memory and grow towards each other.
This can be used to overwrite the return address of a program to let it crash or even execute malicious code by pointing the return address to that code.

\subsubsection{Security Analyses}
For security analyses we can reuse the concept of fault tree, here called attack trees.
The nodes of the tree represent potential threads and can be annotated with different attributes like possibilities, estimated impact or attack cost.
When having an annotated tree, try to resolve the most impactful/probable/\ldots attacks.\\
Another approach is baseline protection.
This is a catalog of what should be done in most situations including components of an IT system, a list of threads and a list of safeguards.
This makes risk analyses only necessary in special cases, for everything else the catalogue is sufficient.

\subsubsection{Trade-offs}
When increasing security, trade-offs have to be met.
First, performance might suffer due to the additional measures of security.
Second, usability can be decreased by introducing barriers like password pages.
Since development efforts are also increased, the time to market is prolonged and costs are increased.

